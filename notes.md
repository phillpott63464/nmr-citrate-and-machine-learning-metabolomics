Likely to work with 5% D2O 95% H2O instead of 100% D2O. It's just cheaper. 

https://pillbuys.com/research/Magnesium%20citrate/17.pdf


[citrate-table](pillbuys-citrate-table.csv)

Initial concentration (mmol dm-3) of:
CM: Metal
CL: Ligand (citrate)
CH: mineral acid

[citrate-formation-table](pillbuys-formation.csv)

$M_qL_PH_R$
S=sum square residuals
R = R factor defined in ref. 54 (A. Sabatini, A. Vacca and P. Gans, Talanta, 21, 53 (1974))
log beta = logarithm of the stability constant for metal-ligand complex. Higher B = more stable product.



https://journals.iucr.org/e/issues/2020/10/00/hb7927/index.html
Evidence for magnesium citrate, not trimagnesium dicitrate, and how to prepare
10mmol citric acid, 10ml water, add 10mmol MgCO3 (actually Mg5(CO3)4(OH)2). Slow fizzing, clear colourless solution, dry at 333K.

Most common magnesium citrates are:
Mg:citrate
1:1
3:2
2:1

https://pubchem.ncbi.nlm.nih.gov/compound/13136
Only calcium citrate I can find is tricalcium dicitrate. Very soluble though.

https://pubs.acs.org/doi/pdf/10.1021/acs.jpcb.0c06377?ref=article_openPDF


---

We have sodium citrate (pH around 8) and citric acid (pH around 0.1) (pubchem)

We can make most solutions we need, therefore, using mixtures of these two

We need to figure out where the maximum of each is (between the pKas, probably, see henderson hasselbatch equation?)

pH = pKa + log ([base]/[acid])

The henderson hassebalch equation does not align with the book's table?

https://www.researchgate.net/figure/Fractions-of-various-citric-acid-H4Cit-species-as-functions-of-pH_fig1_375733298
- Good, nice graph. Was calculated with iron... maybe a problem? Also, at concentrations >2M


You will never get to the same fraction of intermediates as you will with citric acid/trisodium citrate.

So, pHs:
H4Cit = 0.1 (close to 0 as gets, obviously can't reduce further than pure citric acid)
H3Cit- = 3.5
H2Cit2- = 4.5
HCit-3 = 7.6 (final pH of 100% trisodium citrate is roughly 8)

So, distribute 24 samples across that range.

https://www.researchgate.net/figure/a-Speciation-of-citric-acid-as-function-of-pH-b-Speciation-of-lead-in-citrate-water_fig3_354577826
- Similar graph, calculated with lead, 0.1M though, which is better

pHs:
H4Cit = 0.5
H3Cit- = 3.9
H2Cit2- = 5.5
HCit-3 = 8

https://www.researchgate.net/figure/Raman-spectra-of-citric-acid-at-different-pH-from-1800-to-1000-cm-1_fig4_270958347
Very similar to second 


So, since 2/3 agree with them, these are our final options:

pHs:
H4Cit = 0.5
H3Cit- = 3.9
H2Cit2- = 5.5
HCit-3 = 8

We then need to mix the ratios together to make a decent spread.
8-0.5=7.5
7.5/20=0.35
Step by 0.35 for linear, but it's not linear

From the book (data for biochemical research), a step of -4.5/5ml acid, + equal base gives a 0.2 change in pH. 

https://scrippslabs.com/ph-of-common-reagents-at-room-temperature/
- pH of 0.1M citric acid is 2.1

600ul/sample
14,400ul/total
Just how much of this is going to be citrate vs citric acid?
1mM
- [/] Calculate moles
- [/] Calculate grams

[Experiments](experiments.csv) for ratios, [python script](main.py) for calculations (includes stocks), [out.csv](out.csv) for final volumes.
 
Actual requirements:
Acid weight: 13.249mg
Acid volume: 6.306ml
Base weight: 23.804mg
Base volume: 8.094ml

Requirements based upon the validity of a balance:
Acid weight: 14.0mg
Acid volume: 6.663ml
Base weight: 24.0mg
Base volume: 8.16ml

---
[graph](graph.csv), taken from https://www.researchgate.net/figure/Raman-spectra-of-citric-acid-at-different-pH-from-1800-to-1000-cm-1_fig4_270958347, using (graphextractor)[https://apps.automeris.io/wpd4]


---

[graph.py](graph.py) and its output [ratios](ratios.csv) are the ways to generate a synthetic speciation curve. There are 201 sample ratios (step 0.5%), and now we need to decide which 24 we want to put in the sample changer. 

We want to focus on the maxim of each state (taken from [this graph](graph.png) generated by [graph.py](graph.py)):
- H3A: 100% acid (pH2.1)
- H2A-: pH between 3.5 and 4
- HA2-: pH between 5 and 5.5
- A3-: 100% base (pH 8)

And the exchange point:
- pH 3: 50% H3A, 50% H2A-
- pH 4.5: 50% H2A-, 50% HA3-
- pH 6: 50% HA20, 50% A3-

So that's 8/24. So for the rest... guess?

---

# Model Results
## MLPs
### Single metabolite (citrate):
Best Trial Performance (Validation Set):

    R² Score: 1.0000 (Coefficient of determination - measures how well the model explains variance in the data. Range: 0-1, higher is better)
    MAE: 0.006041 (Mean Absolute Error - average absolute difference between predictions and true values. Lower is better)
    RMSE: 0.010638 (Root Mean Square Error - penalizes larger errors more heavily than MAE. Lower is better)

Final Test Set Performance:

    R² Score: 0.9999
    MAE: 0.005869
    RMSE: 0.013923

Best Hyperparameters:

    Number of Epochs: 100
    Batch Size: 90
    Learning Rate: 2.04e-05
    Division Size: 2 (controls network width - smaller values = wider layers)

Model Architecture: Input size → 2 divisions → ... → 1 output Data Split:

    Training: 70%
    Validation: 15% (used for hyperparameter optimization)
    Test: 15% (held out for final evaluation)

Total Trials Completed: 1003

### Multi metabolite (all metabolites):
Best Trial Performance (Validation Set):

    R² Score: 0.9987 (Coefficient of determination - measures how well the model explains variance in the data. Range: 0-1, higher is better)
    MAE: 0.033697 (Mean Absolute Error - average absolute difference between predictions and true values. Lower is better)
    RMSE: 0.053592 (Root Mean Square Error - penalizes larger errors more heavily than MAE. Lower is better)

Final Test Set Performance:

    R² Score: 0.9988
    MAE: 0.033406
    RMSE: 0.052307

Best Hyperparameters:

    Number of Epochs: 160
    Batch Size: 160
    Learning Rate: 9.15e-05
    Division Size: 2 (controls network width - smaller values = wider layers)

Model Architecture: Input size → 2 divisions → ... → 1 output Data Split:

    Training: 70%
    Validation: 15% (used for hyperparameter optimization)
    Test: 15% (held out for final evaluation)

Total Trials Completed: 407

### Metabolite randomisation (no holdout)
Best Trial Performance (Validation Set):

    Combined Score: 0.8350 (0.5 * Accuracy + 0.5 * R², optimized metric)
    Classification Accuracy: 0.7601 (Presence prediction accuracy - higher is better)
    Concentration R²: 0.9100 (Coefficient of determination for concentration - higher is better)
    Concentration MAE: 0.293812 (Mean Absolute Error for concentration - lower is better)
    Concentration RMSE: 0.461827 (Root Mean Square Error for concentration - lower is better)

Final Test Set Performance:

    Classification Accuracy: 0.7532
    Concentration R²: 0.9006
    Concentration MAE: 0.289353
    Concentration RMSE: 0.465910

Best Hyperparameters:

    Number of Epochs: 60
    Batch Size: 60
    Learning Rate: 1.22e-05
    Division Size: 3 (controls network width - smaller values = wider layers)
    Loss Weight: 8.18 (weighting for concentration vs presence loss)

Model Architecture: Input size → 3 divisions → ... → 2 outputs (presence + concentration) Multi-Task Learning:

    Task 1: Binary classification for substance presence (BCEWithLogitsLoss)
    Task 2: Regression for concentration prediction (MSE, weighted by presence)
    Combined Loss: Classification + 8.18 × Concentration Loss

Data Split:

    Training: 70%
    Validation: 15% (used for hyperparameter optimization)
    Test: 15% (held out for final evaluation)

Total Trials Completed: 95

### Metabolite randomisation with holdout after training for an entire weekend:

Best Trial Performance (Validation Set):

    Combined Score: 0.9788 (0.5 * Accuracy + 0.5 * R², optimized metric)
    Classification Accuracy: 0.9821 (Presence prediction accuracy - higher is better)
    Concentration R²: 0.9755 (Coefficient of determination for concentration - higher is better)
    Concentration MAE: 0.148405 (Mean Absolute Error for concentration - lower is better)
    Concentration RMSE: 0.232599 (Root Mean Square Error for concentration - lower is better)

Final Test Set Performance:

    Classification Accuracy: 0.4764
    Concentration R²: -0.2529
    Concentration MAE: 1.110174
    Concentration RMSE: 1.665682

Clearly, overfitting is a major issue. Two ways to improve this:
- Using something smarter than an MLP
- Have the training, testing, and validation data be entirely separate spectra from each other

### Hilbert Transform (FID data) single metabolite
Validation Set (Optimization Target):

    R² Score: 0.999527
    MAE: 0.025928
    RMSE: 0.037472

Test Set (Final Evaluation):

    R² Score: 0.998627
    MAE: 0.031865
    RMSE: 0.051972

Best Parameters: - n_epochs: 100.0 - batch_size: 90.0 - lr: 0.003714545902240392 - div_size: 2.0

It definetly works on FID data at least as well as it does on frequency data, generally with a lower requirement for length of data, as well as not requiring an x axis to reduce training tensor.

#### With better data discard
Validation Set (Optimization Target):

    Combined Score (0.5 * MAE + 0.5 * RMSE): 0.039488
    R² Score: 0.998639
    MAE: 0.028270
    RMSE: 0.050705

Test Set (Final Evaluation):

    R² Score: 0.998807
    MAE: 0.024982
    RMSE: 0.049781


## CNN
### Metabolite Randomisation with holdout 10 trials:

Best Trial Performance (Validation Set):

    Combined Score: 0.5649 (0.5 * Accuracy + 0.5 * R², optimized metric)
    Classification Accuracy: 0.9722 (Presence prediction accuracy - higher is better)
    Concentration R²: 0.1575 (Coefficient of determination for concentration - higher is better)
    Concentration MAE: 1.098701 (Mean Absolute Error for concentration - lower is better)
    Concentration RMSE: 1.587124 (Root Mean Square Error for concentration - lower is better)

Final Test Set Performance:

    Classification Accuracy: 0.5900
    Concentration R²: -0.1577
    Concentration MAE: 1.301418
    Concentration RMSE: 1.774389


Excellent classification, truly terrible concentration. And still terrible  overfitting.

### Hilbert Transform (FID data) single metabolite

SOOOO slow and SOOO bad it's just not even worth it when an MLP can get to 3% mean error in 100 trials in the time it takes the CNN to do 1 trial

## Transformer
